# Enhancing Out-of-Distribution Detection via Boundary-Constrained Generative Adversarial Networks

The core code is located in ./train_bcood.py.

## 1.   Algorithm Overview

The BCOOD method is a novel OOD detection method via Boundary-Constrained Generative Adversarial Networks. The core innovation lies in generating difficult OOD samples that simultaneously satisfy two critical criteria: proximity to low-density areas of in-distribution (ID) data and sufficient diversity in the feature space.



## 2.   Network Architecture

**A.**  **Generator**

Structure:

(1) A fully connected layer upscales the latent vector to an appropriate dimension.

(2) Transposed convolutional layers perform upsampling.

(3) Batch normalization and LeakyReLU activation functions are used. The final output is constrained to the range [-1, 1] using a Tanh activation function.

**B.**  **Discriminator**

Structure:

(1) Convolutional blocks extract features (including convolution, batch normalization, 

LeakyReLU, and Dropout).

(2) Two output heads:
Adversarial head: Determines input authenticity (binary classification).
Auxiliary head: Performs category classification (multi-class classification).



## 3.   Fast Sampling Module

Implementation Key Points:

(1) Use a Gaussian Mixture Model (GMM) to model the latent representation of ID data.

(2) Each Gaussian component corresponds to a semantic category.

(3) Ensure structured representation in the latent space.



## 4.   Difficult OOD Data Generation Module

(1) Introducing FGSM into GAN (Perturbing the Generator’s Output)

(2) Improved FGSM: Adding Directional Parameters to FGSM

(3) Adding FGSM Perturbation to the Latent Space

(4) Difficult OOD Data Generation**



By integrating the three points above—i.e., combining FGSM with GAN (point 1) and designing a diversity enhancement mechanism (points 2 and 3)—we obtain the following two types of difficult OOD data:

(a) DOOD1: Generated by applying FGSM perturbation to the output of the GAN generator.

Data characteristics: Biased towards local diversity, such as features like animal eyes and ears.

 (b) DOOD2: Generated by perturbing the latent space.

Data characteristics: Biased towards global diversity, affecting high-level semantics (e.g., shape and structure).



Note: During training, the values of epsilon_mean and epsilon_std can be adjusted according to different epoch stages. For example, in later epochs, these values should be appropriately increased to achieve greater diversity.



## 5.   Data Conflict Adjustment Module

Distance Correlation Calculation

Subsequently, two types of regularization terms are added to the generator loss:

(1) Latent space-feature space distance preservation: Ensures that the separation in the latent space is transferred to the feature space.

(2) Distance constraints between DOOD1 and AOOD, and DOOD2 and AOOD: Forces the generated difficult OOD data to be close to the real OOD region, thereby achieving the goal of staying away from the ID data region.



## 6.   Partial Parameter Settings

| Parameter            | Suggested Value | Description                                             |
| -------------------- | --------------- | ------------------------------------------------------- |
| latent_dim           | 100             | Latent space dimension                                  |
| batch_size           | 128             | Batch size                                              |
| lr_g, lr_d           | 0.0002          | Generator and discriminator learning  rates             |
| fgsm_epsilon         | 0.01 - 0.05     | Base strength of FGSM perturbation                      |
| lambda_reg           | 0.25            | Strength of distance preservation  regularization       |
| tau                  | 0.15            | GMM density threshold                                   |
| lambda1 (Formula 16) | 0.1 - 0.25      | Strength of DOOD1 distance constraint                   |
| lambda2(Formula 16)  | 0.05 - 0.2      | Strength of DOOD2 distance constraint                   |
| lambda3 (Formula 16) | 0.1 – 0.3       | Strength of latent-feature space distance  preservation |

 

# Program Execution

## 1.   File Structure

(1) configs folder: Stores configuration files, including parameters for some datasets (datasets), BCOOD training parameters, neural network parameters and pre-trained network parameters for comparative methods (networks), training parameters for comparative methods (pipelines), and preprocessing parameters (preprocessors).

(2) data folder: Stores downloaded datasets.

(3) models folder: Trained models are saved here.

(4) results folder: Stores experimental results.

(5) train_bcood.py file: Implementation of the BCOOD method.

## 2.   Code Environment

Experimental environment: Pycharm 2020.2.5 Community Edition, Intel(R) Core(TM) RTX 2080Ti GPU @ 3.00 GHz, 16.0 GB RAM.

python = 3.8.5

Partial dependency versions (all versions refer to the requirements.txt file):

torch==2.1.2

torchvision==0.16.2

torchaudio==2.1.2

numpy==1.26.3

scipy==1.11.4

pandas==2.1.4

scikit-learn==1.3.2

 

One-click installation of all dependencies:

pip install -r requirements.txt

## 3.   Dataset Download

The program will automatically download CIFAR-100 dataset. To manually prepare data, place the 

data in the data folder:

mkdir -p data/dataset_name

Data format: Standard PyTorch Dataset format.

Relevant dataset download URLs:

(1) Fashion-MNIST: https://github.com/zalandoresearch/fashion-mnist/tree/master

(2) CIFAR-10 and CIFAR-100: https://www.cs.toronto.edu/~kriz/cifar.html

(3) TinyImageNet: https://github.com/rmccorm4/Tiny-Imagenet-200

(4) SVHN: http://ufldl.stanford.edu/housenumbers

(5) LSUN: https://github.com/fyu/lsun (Note: We use the resized version of LSUN)

(6) Datasets used in the FS-OOD benchmark: https://github.com/Jingkang50/OpenOOD

 

Note: Each dataset should be placed in its own folder!

## 4.   Training BCOOD

Note: The parameter settings used in the comparative experiments in the paper are written in the 

corresponding YAML files and can be called directly.

 

Example of training BCOOD on the CIFAR-100 dataset (results from the paper):

```python
# When the pre-trained network is DenseNet-BC  
python train_bcood.py --config configs/BCOOD/cifar100_train_DBC.yml  

# When the pre-trained network is ResNet-34  
python train_bcood.py --config configs/BCOOD/cifar100_train_R34.yml 
```



Custom parameters are also supported for training. Use the following method:

```python
# Override parameters in the configuration file  
python train_bcood.py --config configs/BCOOD/cifar100_train_DBC.yml --epochs 250 --batch_size 64  
```

 

## 5.   Evaluation

Note: Our experiments use two benchmarks for evaluation. Therefore:
 **(1) Evaluation on the Traditional Benchmark**

Example for BCOOD:

  ```python
  python ./scripts/eval_ood.py \
      --dataset lsun,svhn \
      --root ./models/BCOOD_models/specific_model
  ```

Parameter description:

--dataset: Specifies the OOD dataset(s) to test. Multiple datasets can be specified, and the average of each evaluation metric will be calculated.

--root: Specifies the path to the trained model.

 The evaluation for other methods used in the experiments is similar—simply change the --root parameter accordingly.

**(2) Evaluation on the FS-OOD Benchmark**
Since the test set is predefined, there is no need to specify the OOD dataset. However, the ID dataset used for training must be specified. For example, for BCOOD trained on the MNIST dataset:

```python
python ./scripts/eval_fsood.py \
    --id-data mnist \
    --root ./models/BCOOD_models/specific_model  
```

Here, the --id-data parameter specifies the ID dataset used for training.

The evaluation for other methods used in the experiments is similar—simply change the --root parameter accordingly.



**Please cite this manuscript when using the associated code, which is directly related to a manuscript submitted to The Visual Computer. We will release it as soon as the paper is published.**


Reference Yan Xian, Hongru Chen, Ke Liu, Hong Yu. Enhancing Out-of-Distribution Detection via Boundary-Constrained Generative Adversarial Networks. The Visual Computer, 2025.
